{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习识别手势\n",
    "=========\n",
    "\n",
    "文件概述\n",
    "===\n",
    "- 视频转图片\n",
    "\n",
    "- 全局变量设置\n",
    "- 图片尺寸伸缩\n",
    "- one hot 编码\n",
    "- DataLoader定义\n",
    "- 定义网络结构\n",
    "- 常规训练、测试、应用    \n",
    "\n",
    "快速开始\n",
    "====\n",
    "- __步骤一：环境配置__\n",
    "\n",
    "    Python2.7+  numpy  \n",
    "    torch torchvision cuda相关gpu配置 : 因人而异,比较麻烦  \n",
    "    有些包pip下载不了就用conda，反之亦然（玄学）  \n",
    "    但是只要下面这三个安装成了就可以生成数据集 \n",
    "    ```bash\n",
    "    pip install opencv-python\n",
    "    pip install opencv-contrib-python\n",
    "    conda install pillow\n",
    "    ```\n",
    "\n",
    "- __步骤二：视频转图片__\n",
    "\n",
    "    执行以上命令，video_src/ 中的视频转为 picture_src/中的图片\n",
    "    \n",
    "- __步骤三：图片压缩__\n",
    "\n",
    "    picture_src/ 中的图片 经过压缩后存入 dataset/train/ 之后就可以把picture_src里面的图片删掉了（自己手动删）  \n",
    "    （lmc2.py文件内可选择存入train 或 test 或 predict)  \n",
    "    偷懒的存入train之后手动copy一部分dataset/train/中的图片放入dataset/test/就行  \n",
    "    训练阶段dataset/predict/是没什么用的  \n",
    "\n",
    "- __步骤四：训练模型__\n",
    "\n",
    "    可以在控制台，看到训练过程\n",
    "\n",
    "- __步骤五：测试模型__\n",
    "\n",
    "    可以在控制台，看到输出的结果（如果准确率较低，可以考虑增加训练数据量，或者修改model）\n",
    "\n",
    "\n",
    "声明\n",
    "===\n",
    "本框架收集自Github上多个其他博主，本人也记不清了，仅作为自用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 必须运行：“全局变量”设置\n",
    "直接运行一遍即可\n",
    "* 所有可能的label列在ALL_CHAR_SET中\n",
    "* 每张图片的label长度列在MAX_CAPTCHA中\n",
    "* 图片大小由IMAGE_HEIGHT 和 IMAGE_WIDTH 确定\n",
    "* train,test,predict三个数据集路径的定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "\n",
    "\n",
    "# 像下面这么写就支持二分类，修改ALL_CHAR_SET就可以修改分类数，支持数字、大写字母、小写字母\n",
    "ALL_CHAR_SET = ['0', '1']\n",
    "ALL_CHAR_SET_LEN = len(ALL_CHAR_SET)\n",
    "MAX_CAPTCHA = 1  # 这里是从一个验证码识别的框架里搞出来的，定义为1的话就是普通分类问题\n",
    "\n",
    "# 图像大小\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH = 64\n",
    "\n",
    "TRAIN_DATASET_PATH = 'dataset' + os.path.sep + 'train'  # 这里是最终训练集的位置\n",
    "TEST_DATASET_PATH = 'dataset' + os.path.sep + 'test'  # 这里是最终测试集的位置\n",
    "PREDICT_DATASET_PATH = 'dataset' + os.path.sep + 'predict'  # 这里随便放点图片，可以看看输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 必须运行：one hot编码设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# 这里进行one hot 编码\n",
    "\n",
    "# 定义工具函数，借助unicode的ord函数把字符串转为数字\n",
    "# 把下划线定义为62，把数字字符定义为自己，把大然后是大写字母，然后是小写字母\n",
    "'''\n",
    "def char2pos(c):\n",
    "    if c == '_':\n",
    "        k = 62\n",
    "        return k\n",
    "    k = ord(c)-48\n",
    "    if k > 9:\n",
    "        k = ord(c) - 65 + 10\n",
    "        if k > 35:\n",
    "            k = ord(c) - 97 + 26 + 10\n",
    "            if k > 61:\n",
    "                raise ValueError('error')\n",
    "    return k\n",
    "'''\n",
    "def char2pos(c):\n",
    "    k = ord(c)\n",
    "    \n",
    "    if 48<= k <=57:  # 把数字映射到0~9\n",
    "        k = k - 48\n",
    "    elif 65<= k <=90:  # 把大写字母映射到10~35\n",
    "        k = k - 65 + 10\n",
    "    elif 97 <= k <= 122: # 把小写字母映射到36~61\n",
    "        k = k - 97 + 10 + 26\n",
    "    elif k == 95:  # 把下划线'_'映射到62\n",
    "        k = 62\n",
    "    else:\n",
    "        raise ValueError('error')\n",
    "    return k\n",
    "    \n",
    "\n",
    "def encode(text):  #字符串到one hot 向量\n",
    "    # 这是个多分类框架，不过单分类也可以做\n",
    "    vector = np.zeros(ALL_CHAR_SET_LEN * MAX_CAPTCHA, dtype=float)\n",
    "    # 遍历字符串中的所有字符，单分类的话其实只循环一次\n",
    "    for i, c in enumerate(text):\n",
    "        idx = i * ALL_CHAR_SET_LEN + char2pos(c)\n",
    "        vector[idx] = 1.0\n",
    "    return vector\n",
    "\n",
    "\n",
    "def decode(vec):  # 把one hot向量变成字符串\n",
    "    char_pos = vec.nonzero()[0]\n",
    "    text = []\n",
    "    for i, c in enumerate(char_pos):\n",
    "        char_at_pos = i  # c/63\n",
    "        char_idx = c % ALL_CHAR_SET_LEN\n",
    "        if char_idx < 10:\n",
    "            char_code = char_idx + ord('0')\n",
    "        elif char_idx <36:\n",
    "            char_code = char_idx - 10 + ord('A')\n",
    "        elif char_idx < 62:\n",
    "            char_code = char_idx - 36 + ord('a')\n",
    "        elif char_idx == 62:\n",
    "            char_code = ord('_')\n",
    "        else:\n",
    "            raise ValueError('error')\n",
    "        text.append(chr(char_code))\n",
    "    return \"\".join(text)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':  # 做点测试\n",
    "    e = encode(\"1\")\n",
    "    print(e)\n",
    "    print(decode(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 视频转图片\n",
    "* 运行本节代码：视频转化为图片，并存放在\"picture_src\"文件夹中，一次不宜太多视频（画质太高占内存）\n",
    "* 如果picture_src文件夹不存在，则会创建该文件夹\n",
    "\n",
    "要求：\n",
    "* 所有要转化成图片的视频都存放在./video_src文件夹中\n",
    "* 所有视频的命名格式为 ‘数字_乱码’ 其中数字用作label  目前为 0:left  1:right\n",
    "* 为了防止重名覆盖，不要把同样的手势分批转化（但是可以是很多个同类手势的视频都放在video_src/中一起运行该程序）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_left0.mp4save_success in:\n",
      "picture_src\n",
      "0_left1.mp4save_success in:\n",
      "picture_src\n",
      "1_right0.mp4save_success in:\n",
      "picture_src\n",
      "1_right1.mp4save_success in:\n",
      "picture_src\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "def save_img():\n",
    "    c = 0\n",
    "    video_path = './video_src'  # 输入的视频放在文件夹./video_src下\n",
    "    folder_name = \"picture_src\"  # 输出图片 到 ./picture_src文件夹里\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    pic_path = folder_name + '/'\n",
    "\n",
    "    videos = os.listdir(video_path)\n",
    "    for video_name in videos:  # 遍历所有视频文件名\n",
    "        sign_name = video_name.split('_')[0]  # 获得文件名第一个下划线前的部分（即手势类型）\n",
    "\n",
    "        vc = cv2.VideoCapture(video_path+'/'+video_name)  # 读取视频\n",
    "        have_next = vc.isOpened()\n",
    "        while have_next:\n",
    "            c = c + 1  # 防止文件重名\n",
    "            have_next, frame = vc.read()  # 读一帧\n",
    "            if have_next:\n",
    "                cv2.imwrite(pic_path + sign_name + '_' + str(c) + '.png', frame)\n",
    "                cv2.waitKey(1)\n",
    "            else:\n",
    "                break\n",
    "        vc.release()\n",
    "        print(video_name + 'save_success in:')\n",
    "        print(folder_name)\n",
    "\n",
    "\n",
    "save_img()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图片尺寸伸缩后 放入数据集源文件夹\n",
    "运行本段代码将./picture_src中的图片调整大小后\n",
    "\n",
    "按一定概率分布放在dataset/train/或者dataset/test/ 文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "# 这里把图片尺寸压缩并存入指定文件夹，\n",
    "# 可以指定为TRAIN || TEST || PREDICT （在第19行）\n",
    "\n",
    "def get_picture_size(inpath):\n",
    "    im = Image.open(inpath)\n",
    "    x, y = im.size\n",
    "    return x, y\n",
    "\n",
    "def resize_picture(inpath):\n",
    "    im = Image.open(inpath)\n",
    "    x, y = im.size\n",
    "    im = im.resize((IMAGE_WIDTH, IMAGE_HEIGHT), Image.ANTIALIAS)  # lmc3里可以调整输出图片的大小（我随便设得64*64）\n",
    "    filename = inpath.split('/')[-1].split('.')[0]\n",
    "    sign_name = filename.split('_')[0]\n",
    "    if random.random() < 0.8:\n",
    "        folder_name = TRAIN_DATASET_PATH  # 这个是输出小图片的文件夹,默认是放到训练集dataset/train/中\n",
    "    else:\n",
    "        folder_name = TEST_DATASET_PATH\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    outpath = folder_name + '/' + filename + '.png'\n",
    "    im.save(outpath)\n",
    "\n",
    "\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "c = 0\n",
    "folder = './picture_src'  # 这里写大图片所在的文件夹\n",
    "pictures = os.listdir(folder)\n",
    "for picture_name in pictures:\n",
    "    picpath = folder + '/' + picture_name\n",
    "    resize_picture(picpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里定义DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 定义预处理操作：转化为灰度值，然后转化为tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# 是torch.utils.data.Dataset的子类\n",
    "class mydataset(Dataset):\n",
    "\n",
    "    def __init__(self, folder, transform=None):\n",
    "        self.train_image_file_paths = [os.path.join(folder, image_file) for image_file in os.listdir(folder)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_image_file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_root = self.train_image_file_paths[idx]\n",
    "        image_name = image_root.split(os.path.sep)[-1]\n",
    "        image = Image.open(image_root)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        label = encode(image_name.split('_')[0])  # 图片命名为\"label值_时间戳.PNG\", 对label值做 one-hot 处理\n",
    "        return image, label  # 返回 tensor+one hot标签 二元组\n",
    "\n",
    "\n",
    "def get_train_data_loader():\n",
    "    dataset = mydataset(TRAIN_DATASET_PATH, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=64, shuffle=True)  # DataLoader第一个参数是Dataset类的\n",
    "\n",
    "\n",
    "def get_test_data_loader():\n",
    "    dataset = mydataset(TEST_DATASET_PATH, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "def get_predict_data_loader():\n",
    "    dataset = mydataset(PREDICT_DATASET_PATH, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=1, shuffle=False)  # 这里就不打乱了，方便顺序对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 这里搭建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# CNN Model (模仿VGG，但因算力不足没有做VGG那么深)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1, stride=2),\n",
    "            # 下面是MobileNet的核心特色，一个deepwise卷积层紧跟着一个pointwise卷积层\n",
    "            nn.Conv2d(32,32, kernel_size=3, padding=1, stride=1, groups=32),\n",
    "            nn.Conv2d(32, 128, kernel_size=1, padding=0, stride=2),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1, groups=128),\n",
    "            nn.Conv2d(128, 256, kernel_size=1, padding=0, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1, groups=256),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Dropout(0.5),  # drop 50% of the neuron,这个是我自己加的...反正已经是阉割版了，意思到了即可\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, MAX_CAPTCHA * ALL_CHAR_SET_LEN),  # 最后输出长为MAX_CAPTCHA * ALL_CHAR_SET_LEN的向量\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init net\n",
      "epoch: 0 step: 9 loss: 0.6209810972213745\n",
      "epoch: 0 step: 19 loss: 0.5489240884780884\n",
      "epoch: 0 step: 29 loss: 0.4909036159515381\n",
      "epoch: 0 step: 39 loss: 0.40799814462661743\n",
      "epoch: 0 step: 49 loss: 0.33559703826904297\n",
      "epoch: 0 step: 59 loss: 0.24925178289413452\n",
      "epoch: 0 step: 69 loss: 0.20021595060825348\n",
      "epoch: 0 step: 79 loss: 0.14480940997600555\n",
      "epoch: 0 step: 89 loss: 0.13387715816497803\n",
      "epoch: 0 step: 89 loss: 0.13387715816497803\n",
      "epoch: 1 step: 9 loss: 0.10757225006818771\n",
      "epoch: 1 step: 19 loss: 0.10288936644792557\n",
      "epoch: 1 step: 29 loss: 0.06859497725963593\n",
      "epoch: 1 step: 39 loss: 0.08062350749969482\n",
      "epoch: 1 step: 49 loss: 0.05666393041610718\n",
      "epoch: 1 step: 59 loss: 0.08754513412714005\n",
      "epoch: 1 step: 69 loss: 0.07993736863136292\n",
      "epoch: 1 step: 79 loss: 0.043985359370708466\n",
      "epoch: 1 step: 89 loss: 0.05890776589512825\n",
      "epoch: 1 step: 89 loss: 0.05890776589512825\n",
      "epoch: 2 step: 9 loss: 0.0261266827583313\n",
      "epoch: 2 step: 19 loss: 0.05119902640581131\n",
      "epoch: 2 step: 29 loss: 0.0397038534283638\n",
      "epoch: 2 step: 39 loss: 0.055063456296920776\n",
      "epoch: 2 step: 49 loss: 0.030644135549664497\n",
      "epoch: 2 step: 59 loss: 0.02973959408700466\n",
      "epoch: 2 step: 69 loss: 0.03670918568968773\n",
      "epoch: 2 step: 79 loss: 0.05065286532044411\n",
      "epoch: 2 step: 89 loss: 0.01781226508319378\n",
      "epoch: 2 step: 89 loss: 0.01781226508319378\n",
      "epoch: 3 step: 9 loss: 0.009872911497950554\n",
      "epoch: 3 step: 19 loss: 0.010855371132493019\n",
      "epoch: 3 step: 29 loss: 0.032813265919685364\n",
      "epoch: 3 step: 39 loss: 0.01580081507563591\n",
      "epoch: 3 step: 49 loss: 0.01753544807434082\n",
      "epoch: 3 step: 59 loss: 0.01146305538713932\n",
      "epoch: 3 step: 69 loss: 0.016859985888004303\n",
      "epoch: 3 step: 79 loss: 0.01264863833785057\n",
      "epoch: 3 step: 89 loss: 0.012509547173976898\n",
      "epoch: 3 step: 89 loss: 0.012509547173976898\n",
      "epoch: 4 step: 9 loss: 0.013147770427167416\n",
      "epoch: 4 step: 19 loss: 0.00859113596379757\n",
      "epoch: 4 step: 29 loss: 0.010275071486830711\n",
      "epoch: 4 step: 39 loss: 0.010296447202563286\n",
      "epoch: 4 step: 49 loss: 0.0071169538423419\n",
      "epoch: 4 step: 59 loss: 0.019828226417303085\n",
      "epoch: 4 step: 69 loss: 0.010113302618265152\n",
      "epoch: 4 step: 79 loss: 0.009002582170069218\n",
      "epoch: 4 step: 89 loss: 0.020503811538219452\n",
      "epoch: 4 step: 89 loss: 0.020503811538219452\n",
      "epoch: 5 step: 9 loss: 0.02718266099691391\n",
      "epoch: 5 step: 19 loss: 0.019006503745913506\n",
      "epoch: 5 step: 29 loss: 0.00669986754655838\n",
      "epoch: 5 step: 39 loss: 0.004029261413961649\n",
      "epoch: 5 step: 49 loss: 0.013914957642555237\n",
      "epoch: 5 step: 59 loss: 0.007769138552248478\n",
      "epoch: 5 step: 69 loss: 0.007050676271319389\n",
      "epoch: 5 step: 79 loss: 0.010391581803560257\n",
      "epoch: 5 step: 89 loss: 0.014976277947425842\n",
      "epoch: 5 step: 89 loss: 0.014976277947425842\n",
      "epoch: 6 step: 9 loss: 0.0444372221827507\n",
      "epoch: 6 step: 19 loss: 0.020023774355649948\n",
      "epoch: 6 step: 29 loss: 0.004903472028672695\n",
      "epoch: 6 step: 39 loss: 0.012454480864107609\n",
      "epoch: 6 step: 49 loss: 0.0031583099626004696\n",
      "epoch: 6 step: 59 loss: 0.014800852164626122\n",
      "epoch: 6 step: 69 loss: 0.011123934760689735\n",
      "epoch: 6 step: 79 loss: 0.008868401870131493\n",
      "epoch: 6 step: 89 loss: 0.01380277331918478\n",
      "epoch: 6 step: 89 loss: 0.01380277331918478\n",
      "epoch: 7 step: 9 loss: 0.005852332804352045\n",
      "epoch: 7 step: 19 loss: 0.004708339925855398\n",
      "epoch: 7 step: 29 loss: 0.00424475222826004\n",
      "epoch: 7 step: 39 loss: 0.006776000838726759\n",
      "epoch: 7 step: 49 loss: 0.007742315996438265\n",
      "epoch: 7 step: 59 loss: 0.00892294105142355\n",
      "epoch: 7 step: 69 loss: 0.004146035294979811\n",
      "epoch: 7 step: 79 loss: 0.005236577708274126\n",
      "epoch: 7 step: 89 loss: 0.0023146476596593857\n",
      "epoch: 7 step: 89 loss: 0.0023146476596593857\n",
      "epoch: 8 step: 9 loss: 0.0032857349142432213\n",
      "epoch: 8 step: 19 loss: 0.010564609430730343\n",
      "epoch: 8 step: 29 loss: 0.003376482054591179\n",
      "epoch: 8 step: 39 loss: 0.014615763910114765\n",
      "epoch: 8 step: 49 loss: 0.0041993483901023865\n",
      "epoch: 8 step: 59 loss: 0.006242194212973118\n",
      "epoch: 8 step: 69 loss: 0.007398666348308325\n",
      "epoch: 8 step: 79 loss: 0.00699635362252593\n",
      "epoch: 8 step: 89 loss: 0.01320081390440464\n",
      "epoch: 8 step: 89 loss: 0.01320081390440464\n",
      "epoch: 9 step: 9 loss: 0.0032113995403051376\n",
      "epoch: 9 step: 19 loss: 0.0031642212998121977\n",
      "epoch: 9 step: 29 loss: 0.006169134750962257\n",
      "epoch: 9 step: 39 loss: 0.0032114950008690357\n",
      "epoch: 9 step: 49 loss: 0.0017238914733752608\n",
      "epoch: 9 step: 59 loss: 0.006445988081395626\n",
      "epoch: 9 step: 69 loss: 0.0027853096835315228\n",
      "epoch: 9 step: 79 loss: 0.007079137954860926\n",
      "epoch: 9 step: 89 loss: 0.002734519075602293\n",
      "epoch: 9 step: 89 loss: 0.002734519075602293\n",
      "epoch: 10 step: 9 loss: 0.006145025137811899\n",
      "epoch: 10 step: 19 loss: 0.0026984456926584244\n",
      "epoch: 10 step: 29 loss: 0.004811384715139866\n",
      "epoch: 10 step: 39 loss: 0.0036711222492158413\n",
      "epoch: 10 step: 49 loss: 0.0019608987495303154\n",
      "epoch: 10 step: 59 loss: 0.007058602292090654\n",
      "epoch: 10 step: 69 loss: 0.022133365273475647\n",
      "epoch: 10 step: 79 loss: 0.00287087494507432\n",
      "epoch: 10 step: 89 loss: 0.004124077036976814\n",
      "epoch: 10 step: 89 loss: 0.004124077036976814\n",
      "epoch: 11 step: 9 loss: 0.0034699817188084126\n",
      "epoch: 11 step: 19 loss: 0.0015707446727901697\n",
      "epoch: 11 step: 29 loss: 0.0018569373060017824\n",
      "epoch: 11 step: 39 loss: 0.004254348110407591\n",
      "epoch: 11 step: 49 loss: 0.002645112108439207\n",
      "epoch: 11 step: 59 loss: 0.001269579166546464\n",
      "epoch: 11 step: 69 loss: 0.006201367825269699\n",
      "epoch: 11 step: 79 loss: 0.0020951582118868828\n",
      "epoch: 11 step: 89 loss: 0.0013865510700270534\n",
      "epoch: 11 step: 89 loss: 0.0013865510700270534\n",
      "epoch: 12 step: 9 loss: 0.00616724556311965\n",
      "epoch: 12 step: 19 loss: 0.002667320193722844\n",
      "epoch: 12 step: 29 loss: 0.02168198488652706\n",
      "epoch: 12 step: 39 loss: 0.013329565525054932\n",
      "epoch: 12 step: 49 loss: 0.0033957636915147305\n",
      "epoch: 12 step: 59 loss: 0.005818694829940796\n",
      "epoch: 12 step: 69 loss: 0.0057520680129528046\n",
      "epoch: 12 step: 79 loss: 0.001969978678971529\n",
      "epoch: 12 step: 89 loss: 0.0003787611494772136\n",
      "epoch: 12 step: 89 loss: 0.0003787611494772136\n",
      "epoch: 13 step: 9 loss: 0.0009022544254548848\n",
      "epoch: 13 step: 19 loss: 0.0022607087157666683\n",
      "epoch: 13 step: 29 loss: 0.000721326214261353\n",
      "epoch: 13 step: 39 loss: 0.0023742106277495623\n",
      "epoch: 13 step: 49 loss: 0.0027069919742643833\n",
      "epoch: 13 step: 59 loss: 0.0016189345624297857\n",
      "epoch: 13 step: 69 loss: 0.004020442720502615\n",
      "epoch: 13 step: 79 loss: 0.004588245879858732\n",
      "epoch: 13 step: 89 loss: 0.0006507060606963933\n",
      "epoch: 13 step: 89 loss: 0.0006507060606963933\n",
      "epoch: 14 step: 9 loss: 0.0016611367464065552\n",
      "epoch: 14 step: 19 loss: 0.006742397323250771\n",
      "epoch: 14 step: 29 loss: 0.0012463508173823357\n",
      "epoch: 14 step: 39 loss: 0.00048287794925272465\n",
      "epoch: 14 step: 49 loss: 0.0007785651250742376\n",
      "epoch: 14 step: 59 loss: 0.0020717442966997623\n",
      "epoch: 14 step: 69 loss: 0.0011396388290449977\n",
      "epoch: 14 step: 79 loss: 0.002741449512541294\n",
      "epoch: 14 step: 89 loss: 0.0017180744325742126\n",
      "epoch: 14 step: 89 loss: 0.0017180744325742126\n",
      "epoch: 15 step: 9 loss: 0.001992317382246256\n",
      "epoch: 15 step: 19 loss: 0.007334494031965733\n",
      "epoch: 15 step: 29 loss: 0.005918072070926428\n",
      "epoch: 15 step: 39 loss: 0.000764222233556211\n",
      "epoch: 15 step: 49 loss: 0.001287994789890945\n",
      "epoch: 15 step: 59 loss: 0.010615759529173374\n",
      "epoch: 15 step: 69 loss: 0.0015720874071121216\n",
      "epoch: 15 step: 79 loss: 0.0012664503883570433\n",
      "epoch: 15 step: 89 loss: 0.0006442070007324219\n",
      "epoch: 15 step: 89 loss: 0.0006442070007324219\n",
      "epoch: 16 step: 9 loss: 0.0013709231279790401\n",
      "epoch: 16 step: 19 loss: 0.0003485663910396397\n",
      "epoch: 16 step: 29 loss: 0.0008286320371553302\n",
      "epoch: 16 step: 39 loss: 0.002178353723138571\n",
      "epoch: 16 step: 49 loss: 0.002227662829682231\n",
      "epoch: 16 step: 59 loss: 0.0026376964524388313\n",
      "epoch: 16 step: 69 loss: 0.0031927917152643204\n",
      "epoch: 16 step: 79 loss: 0.0003995537117589265\n",
      "epoch: 16 step: 89 loss: 0.0017916933866217732\n",
      "epoch: 16 step: 89 loss: 0.0017916933866217732\n",
      "epoch: 17 step: 9 loss: 0.0018302646931260824\n",
      "epoch: 17 step: 19 loss: 0.00393755454570055\n",
      "epoch: 17 step: 29 loss: 0.0006635666941292584\n",
      "epoch: 17 step: 39 loss: 0.001133125158958137\n",
      "epoch: 17 step: 49 loss: 0.0014304625801742077\n",
      "epoch: 17 step: 59 loss: 0.0006697764620184898\n",
      "epoch: 17 step: 69 loss: 0.013261616230010986\n",
      "epoch: 17 step: 79 loss: 0.0006234684842638671\n",
      "epoch: 17 step: 89 loss: 0.0010695405071601272\n",
      "epoch: 17 step: 89 loss: 0.0010695405071601272\n",
      "epoch: 18 step: 9 loss: 0.000933568924665451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 step: 19 loss: 0.0025314721278846264\n",
      "epoch: 18 step: 29 loss: 0.0077795591205358505\n",
      "epoch: 18 step: 39 loss: 0.00398209085687995\n",
      "epoch: 18 step: 49 loss: 0.00116478162817657\n",
      "epoch: 18 step: 59 loss: 0.0006718744407407939\n",
      "epoch: 18 step: 69 loss: 0.00034754647640511394\n",
      "epoch: 18 step: 79 loss: 0.0034520013723522425\n",
      "epoch: 18 step: 89 loss: 0.0007643852732144296\n",
      "epoch: 18 step: 89 loss: 0.0007643852732144296\n",
      "epoch: 19 step: 9 loss: 0.000986177008599043\n",
      "epoch: 19 step: 19 loss: 0.0006843197625130415\n",
      "epoch: 19 step: 29 loss: 0.004930164664983749\n",
      "epoch: 19 step: 39 loss: 0.0011308719404041767\n",
      "epoch: 19 step: 49 loss: 0.004874060396105051\n",
      "epoch: 19 step: 59 loss: 0.0004722331650555134\n",
      "epoch: 19 step: 69 loss: 0.0002715354785323143\n",
      "epoch: 19 step: 79 loss: 0.001906530000269413\n",
      "epoch: 19 step: 89 loss: 0.006409444846212864\n",
      "epoch: 19 step: 89 loss: 0.006409444846212864\n",
      "epoch: 20 step: 9 loss: 0.0032792892307043076\n",
      "epoch: 20 step: 19 loss: 0.0015005844179540873\n",
      "epoch: 20 step: 29 loss: 0.00022207308211363852\n",
      "epoch: 20 step: 39 loss: 0.0017064897110685706\n",
      "epoch: 20 step: 49 loss: 0.000491793267428875\n",
      "epoch: 20 step: 59 loss: 0.00045831003808416426\n",
      "epoch: 20 step: 69 loss: 0.0003512343391776085\n",
      "epoch: 20 step: 79 loss: 0.00022984146198723465\n",
      "epoch: 20 step: 89 loss: 0.0014263074845075607\n",
      "epoch: 20 step: 89 loss: 0.0014263074845075607\n",
      "epoch: 21 step: 9 loss: 0.0003070006496272981\n",
      "epoch: 21 step: 19 loss: 0.0020735766738653183\n",
      "epoch: 21 step: 29 loss: 0.0008360700448974967\n",
      "epoch: 21 step: 39 loss: 0.00042402365943416953\n",
      "epoch: 21 step: 49 loss: 0.0016365897608920932\n",
      "epoch: 21 step: 59 loss: 0.0013327396009117365\n",
      "epoch: 21 step: 69 loss: 0.003029880579560995\n",
      "epoch: 21 step: 79 loss: 0.0008237532456405461\n",
      "epoch: 21 step: 89 loss: 0.0003834902890957892\n",
      "epoch: 21 step: 89 loss: 0.0003834902890957892\n",
      "epoch: 22 step: 9 loss: 0.00039249457768164575\n",
      "epoch: 22 step: 19 loss: 0.0002823347458615899\n",
      "epoch: 22 step: 29 loss: 0.00031334487721323967\n",
      "epoch: 22 step: 39 loss: 0.000830810924526304\n",
      "epoch: 22 step: 49 loss: 0.00048275318113155663\n",
      "epoch: 22 step: 59 loss: 0.0023761512711644173\n",
      "epoch: 22 step: 69 loss: 0.0011652205139398575\n",
      "epoch: 22 step: 79 loss: 0.0005485649453476071\n",
      "epoch: 22 step: 89 loss: 0.000405055470764637\n",
      "epoch: 22 step: 89 loss: 0.000405055470764637\n",
      "epoch: 23 step: 9 loss: 0.0002797678462229669\n",
      "epoch: 23 step: 19 loss: 0.0008403719984926283\n",
      "epoch: 23 step: 29 loss: 0.00047800008906051517\n",
      "epoch: 23 step: 39 loss: 0.000618424266576767\n",
      "epoch: 23 step: 49 loss: 0.00020441185915842652\n",
      "epoch: 23 step: 59 loss: 0.003198234597221017\n",
      "epoch: 23 step: 69 loss: 0.00081784394569695\n",
      "epoch: 23 step: 79 loss: 0.0007740496657788754\n",
      "epoch: 23 step: 89 loss: 0.0012229052372276783\n",
      "epoch: 23 step: 89 loss: 0.0012229052372276783\n",
      "epoch: 24 step: 9 loss: 0.0009077045833691955\n",
      "epoch: 24 step: 19 loss: 0.0003163966175634414\n",
      "epoch: 24 step: 29 loss: 0.00033138348953798413\n",
      "epoch: 24 step: 39 loss: 0.00043435144471004605\n",
      "epoch: 24 step: 49 loss: 0.00014816311886534095\n",
      "epoch: 24 step: 59 loss: 0.00022496114252135158\n",
      "epoch: 24 step: 69 loss: 0.00014983068103902042\n",
      "epoch: 24 step: 79 loss: 0.001910804770886898\n",
      "epoch: 24 step: 89 loss: 0.000231873695156537\n",
      "epoch: 24 step: 89 loss: 0.000231873695156537\n",
      "epoch: 25 step: 9 loss: 0.002178368391469121\n",
      "epoch: 25 step: 19 loss: 0.0003086934157181531\n",
      "epoch: 25 step: 29 loss: 0.0009969895472750068\n",
      "epoch: 25 step: 39 loss: 0.0035556501243263483\n",
      "epoch: 25 step: 49 loss: 0.021067332476377487\n",
      "epoch: 25 step: 59 loss: 0.002979431301355362\n",
      "epoch: 25 step: 69 loss: 0.0010035850573331118\n",
      "epoch: 25 step: 79 loss: 0.0006744359852746129\n",
      "epoch: 25 step: 89 loss: 0.0020136882085353136\n",
      "epoch: 25 step: 89 loss: 0.0020136882085353136\n",
      "epoch: 26 step: 9 loss: 0.0009480117005296052\n",
      "epoch: 26 step: 19 loss: 0.0021303086541593075\n",
      "epoch: 26 step: 29 loss: 0.0007673529325984418\n",
      "epoch: 26 step: 39 loss: 0.0010953230084851384\n",
      "epoch: 26 step: 49 loss: 0.0012051628436893225\n",
      "epoch: 26 step: 59 loss: 0.000443883502157405\n",
      "epoch: 26 step: 69 loss: 0.00031991099240258336\n",
      "epoch: 26 step: 79 loss: 0.0003336294030304998\n",
      "epoch: 26 step: 89 loss: 0.0004054764285683632\n",
      "epoch: 26 step: 89 loss: 0.0004054764285683632\n",
      "epoch: 27 step: 9 loss: 0.0013422868214547634\n",
      "epoch: 27 step: 19 loss: 0.002505130134522915\n",
      "epoch: 27 step: 29 loss: 0.0005112650687806308\n",
      "epoch: 27 step: 39 loss: 0.00046106299851089716\n",
      "epoch: 27 step: 49 loss: 0.0013645419385284185\n",
      "epoch: 27 step: 59 loss: 0.0005255412543192506\n",
      "epoch: 27 step: 69 loss: 0.0006908253999426961\n",
      "epoch: 27 step: 79 loss: 0.0005992218502797186\n",
      "epoch: 27 step: 89 loss: 0.001761330058798194\n",
      "epoch: 27 step: 89 loss: 0.001761330058798194\n",
      "epoch: 28 step: 9 loss: 0.0001450980780646205\n",
      "epoch: 28 step: 19 loss: 0.0013584477128461003\n",
      "epoch: 28 step: 29 loss: 0.000977831892669201\n",
      "epoch: 28 step: 39 loss: 0.00010501610086066648\n",
      "epoch: 28 step: 49 loss: 0.00033082699519582093\n",
      "epoch: 28 step: 59 loss: 0.00010603481496218592\n",
      "epoch: 28 step: 69 loss: 0.0005169025389477611\n",
      "epoch: 28 step: 79 loss: 0.00016042959759943187\n",
      "epoch: 28 step: 89 loss: 0.00018165876099374145\n",
      "epoch: 28 step: 89 loss: 0.00018165876099374145\n",
      "epoch: 29 step: 9 loss: 0.00038419890915974975\n",
      "epoch: 29 step: 19 loss: 0.0003564419748727232\n",
      "epoch: 29 step: 29 loss: 7.863061910029501e-05\n",
      "epoch: 29 step: 39 loss: 0.0006729470915161073\n",
      "epoch: 29 step: 49 loss: 0.0015741748502478004\n",
      "epoch: 29 step: 59 loss: 0.00046201207442209125\n",
      "epoch: 29 step: 69 loss: 0.0007223309366963804\n",
      "epoch: 29 step: 79 loss: 0.0010952691081911325\n",
      "epoch: 29 step: 89 loss: 6.913374090800062e-05\n",
      "epoch: 29 step: 89 loss: 6.913374090800062e-05\n",
      "save last model\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 可调超参\n",
    "num_epochs = 30\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "def main():\n",
    "    cnn = CNN()  # 准备模型\n",
    "    cnn = cnn.to(\"cuda\")  # 送给cuda\n",
    "    cnn.train()  # 设置为训练模式\n",
    "    print('init net')\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()  # 准备一个损失函数\n",
    "    optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)  # 准备一个优化器\n",
    "\n",
    "    # Train the Model\n",
    "    train_dataloader = get_train_data_loader()  # 准备数据加载器\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_dataloader):  # 训练循环\n",
    "            images = images.to(\"cuda\")  # X_train送给cuda\n",
    "            labels = labels.to(\"cuda\")  # y_train送给cuda\n",
    "            images = Variable(images)   # 设置为核心类“Variable”，此后对他进行处理\n",
    "            labels = Variable(labels.float())  # 设置为核心类，并调用float()\n",
    "            predict_labels = cnn(images)  # 过神经网络\n",
    "            # print(predict_labels.type)\n",
    "            # print(labels.type)\n",
    "            loss = criterion(predict_labels, labels)  # 计算损失函数（利用此前准备的criterion）\n",
    "            optimizer.zero_grad()  # 清空梯度缓存\n",
    "            loss.backward()  # 计算新的梯度\n",
    "            optimizer.step()  # 利用计算的梯度更新模型\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(\"epoch:\", epoch, \"step:\", i, \"loss:\", loss.item())  # 每过10个mini batch输出一次\n",
    "            if (i+1) % 100 == 0:\n",
    "                torch.save(cnn.state_dict(), \"./model.pkl\")   #每过100个mini batch保存一次model\n",
    "                print(\"save model\")\n",
    "        print(\"epoch:\", epoch, \"step:\", i, \"loss:\", loss.item())  # 每过一个epoch输出一次loss\n",
    "    torch.save(cnn.state_dict(), \"./model.pkl\")   #current is model.pkl保存最终模型\n",
    "    print(\"save last model\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test测试集评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cnn net.\n",
      "Test Accuracy of the model on the 200 test images: 100.000000 %\n",
      "Test Accuracy of the model on the 400 test images: 100.000000 %\n",
      "Test Accuracy of the model on the 600 test images: 100.000000 %\n",
      "Test Accuracy of the model on the 800 test images: 100.000000 %\n",
      "Test Accuracy of the model on the 1000 test images: 100.000000 %\n",
      "Test Accuracy of the model on the 1200 test images: 100.000000 %\n",
      "Test Accuracy of the model on the 1400 test images: 100.000000 %\n",
      "Test Accuracy of the model on the 1529 test images: 100.000000 %\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def main():\n",
    "    cnn = CNN()\n",
    "    cnn.eval()\n",
    "    cnn.load_state_dict(torch.load('model.pkl'))\n",
    "    print(\"load cnn net.\")\n",
    "\n",
    "    test_dataloader = get_test_data_loader()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, (images, labels) in enumerate(test_dataloader):  # 被enumerate的时候会调用覆写的__getitem__\n",
    "        image = images\n",
    "        vimage = Variable(image)\n",
    "        predict_label = cnn(vimage)\n",
    "\n",
    "        c0 = ALL_CHAR_SET[np.argmax(predict_label[0, 0:ALL_CHAR_SET_LEN].data.numpy())]\n",
    "\n",
    "        predict_label = '%s' % (c0)\n",
    "        true_label = decode(labels.numpy()[0])\n",
    "        total += labels.size(0)\n",
    "        if predict_label == true_label:\n",
    "            correct += 1\n",
    "        if total % 200 == 0:\n",
    "            print('Test Accuracy of the model on the %d test images: %f %%' % (total, 100 * correct / total))\n",
    "    print('Test Accuracy of the model on the %d test images: %f %%' % (total, 100 * correct / total))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实际使用一下我们的模型\n",
    "需要我们手动将一些64x64的图片放到dataset/predict/文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load cnn net.\n",
      "No 0   sign= 0\n",
      "No 1   sign= 0\n",
      "No 2   sign= 0\n",
      "No 3   sign= 0\n",
      "No 4   sign= 0\n",
      "No 5   sign= 1\n",
      "No 6   sign= 1\n",
      "No 7   sign= 1\n",
      "No 8   sign= 1\n",
      "No 9   sign= 1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def main():\n",
    "    cnn = CNN()\n",
    "    cnn.eval()\n",
    "    cnn.load_state_dict(torch.load('model.pkl'))\n",
    "    print(\"load cnn net.\")\n",
    "\n",
    "    predict_dataloader = get_predict_data_loader()\n",
    "\n",
    "    for i, (images, labels) in enumerate(predict_dataloader):\n",
    "        image = images\n",
    "        vimage = Variable(image)\n",
    "        predict_label = cnn(vimage)\n",
    "\n",
    "        c0 = ALL_CHAR_SET[np.argmax(predict_label[0, 0:ALL_CHAR_SET_LEN].data.numpy())]\n",
    "        # c1 = ALL_CHAR_SET[np.argmax(predict_label[0, ALL_CHAR_SET_LEN:2 * ALL_CHAR_SET_LEN].data.numpy())]\n",
    "        # c2 = ALL_CHAR_SET[np.argmax(predict_label[0, 2 * ALL_CHAR_SET_LEN:3 * ALL_CHAR_SET_LEN].data.numpy())]\n",
    "        # c3 = ALL_CHAR_SET[np.argmax(predict_label[0, 3 * ALL_CHAR_SET_LEN:4 * ALL_CHAR_SET_LEN].data.numpy())]\n",
    "\n",
    "        c = '%s' % (c0)  # 这个本来是个四重分类框架，现在就一重了，所以这里看起来很多余...\n",
    "        print('No',i,'  sign=',c)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题很弱，效果很好~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
